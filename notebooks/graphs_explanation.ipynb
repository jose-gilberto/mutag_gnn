{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac31100e",
   "metadata": {},
   "source": [
    "## Graph Neural Networks (GNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d9a88",
   "metadata": {},
   "source": [
    "### Representação de um Grafo\n",
    "\n",
    "Matematicamente dado um grafo $G$ podemos defini-lo com um conjunto de vértices $V$ e um conjunto de arestas $E$, tal que o grafo pode ser definido com a seguinte notação: $G = (V, E)$. Cada aresta é composta por um par de vértices, representando a ligação entre eles.\n",
    "\n",
    "<img src='./assets/example_graph.svg'/>\n",
    "\n",
    "Nesse exemplo acima, temos como os vértices $V = (1, 2, 3, 4)$ e as arestas $E = {(1, 2), (2, 3), (2, 4), (3, 4)}$. Consideramos o grafo não direcionado, e o que isso quer dizer? Que o par de vértices, ou aresta, $(1, 2)$ é igual a $(2, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee51824",
   "metadata": {},
   "source": [
    "Mas e agora, como podemos representar um grafo computacionamente? Há duas formas comumente utilizadas para representar as arestas de um grafo computacionalmente: uma matriz de adjacência ou uma lista de pares com os índices dos vértices. Enquanto para os vértices basta uma lista com seus índices e/ou propriedades.\n",
    "\n",
    "Em aplicações os vértices e as arestas podem possuir $n$ propriedades, além de, no caso das arestas, poderem ser direcionadas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8353688",
   "metadata": {},
   "source": [
    "Uma matriz de adjacência é uma matriz quadrada com o número de linhas e colunas sendo iguais ao número de vértices. Ela informa se o vértice $i$ possui uma conexão com o vértice $j$. Sendo assim a posição $A_{ij}$ da matriz $A$ indica se os vértices $i$ e $j$ possuem alguma conexão. No caso de uma conexão entre esses vértices, a posição da matriz tem o valor $1$ atribuido, indicando essa conexão, caso contrário, é atribuído o valor $0$. Nos casos de um grafo não direcionados, a matriz $A$ será sempre uma matriz simétrica.\n",
    "\n",
    "Para o grafo do exemplo acima temos a seguinte matriz de adjacência $A$:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "    0 & 1 & 0 & 0\\\\\n",
    "    1 & 0 & 1 & 1\\\\\n",
    "    0 & 1 & 0 & 1\\\\\n",
    "    0 & 1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5703d155",
   "metadata": {},
   "source": [
    "Enquanto expressar as arestas de um grafo por uma lista de pares de vetores é mais eficiente do ponto de vista computacional, expressar essas arestas por meio de uma matriz de adjacência pode ser mais intuitivo para humanos e mais fácil de implementar. Podemos utilizar também uma lista de arestas para definir uma matriz de adjacência esparsa com a qual podemos trabalhar como se fosse uma matriz densa, mas permitindo operações mais otimizadas em memória. O pacote `torch.sparse` possibilita trabalhar desta forma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2532e",
   "metadata": {},
   "source": [
    "### Graph Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce7f05",
   "metadata": {},
   "source": [
    "*Graph Convolutional Networks* foram introduzidas por [Kipg et al.](https://openreview.net/pdf?id=SJU4ayYgl) em 2016. Ele também escreveu um post em seu [blog](https://tkipf.github.io/graph-convolutional-networks/) sobre esse tipo de redes neurais. As GCNs são semelhantes as convoluções em imagens, uma vez que, os filtros são normalmente compartilhados por todos os locais do grafo. Da mesma forma, as GCNs contam com métodos de passagem de mensagens, o que significa que os vértices trocam informações com os vizinhos e enviam \"mensagens\" entre si. Antes de visualizar a matemática, podemos tentar entender como as GCNs funcionam. O primeiro passo é que cada vértice crie um vetor de recursos que representa a mensagem que deseja enviar a todos os seus vértices vizinhos. Na segunda etapa, as mensagens são enviadas aos vértices vizinhos, de forma que cada vértice receba uma mensagem para cada vizinho que possuir. Abaixo, podemos visualizar as duas etapas no grafo de exemplo.\n",
    "\n",
    "![messagem dos vértices](./assets/graph_message_passing.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65446c53",
   "metadata": {},
   "source": [
    "Se queremos representar isso de forma matemática, primeiros precisamos decidir como combinar todas as mensagens recebidas pelos vértices. Como o número de mensagens varia ao longo de todo o grafo, precisamos de uma operação que funcione para qualquer número de mensagens. Sendo assim, uma maneira usual de realizar isso é através da soma ou da média. Dada as *features* anteriores dos vértices $H^{(l)}$, a camada GCN é definida como:\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma(\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}H^{(l)}W^{(l)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af45f3a",
   "metadata": {},
   "source": [
    "$W^{(l)}$ são os pesos com os quais transformamos nossas *features* de entrada em mensagens ($H^{(l)}W^{(l)}$). Adicionamos então a matriz de identidade a matriz de adjacência $A$, de forma que, cada vértice envie uma mensagem também para si mesmo: $\\hat{A} = A + I$. Finalmente, para tirar a média ao invés de somar, calculamos a matriz $\\hat{D}$, que é uma matriz diagonal com os elementos $D_{ii}$ iguais ao número de vizinhos que o vértice $i$ possui. $\\sigma$ representa uma função de ativação arbitrária, e não necessariamente uma sigmoid (normalmente são utilizadas ReLU em GCNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440fa8ef",
   "metadata": {},
   "source": [
    "Quando implementamos uma camda GCN no PyTorch, podemos utilizar as operações com tensors. Ao invés de definir uma matriz $\\hat{D}$, podemos simplesmente dividir o número de mensagens pelo número de vizinhos posteriormente. Além disso, substituímos a matriz de pesos por uma camada Linear que também permite adicionar um bias. Podemos escrever um módulo GCN em PyTorch da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8eb2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(c_in, c_out)\n",
    "    \n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Tensor com as features do shape de um vértice (batch_size, num_nodes, c_in).\n",
    "            adj_matrix - Batch de matrizes de adjacência do grafo. Se houver uma matriz de adjacência de i para j\n",
    "                         adj_matriz[b,i,j] = 1 else 0. Suporta arestas direcionadas com matrizes são simétricas. Presume\n",
    "                         que as conexões da matriz identidade já foram adicionadas.\n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
    "        node_feats = node_feats / num_neighbours\n",
    "        return node_feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81146bf8",
   "metadata": {},
   "source": [
    "Para entender melhor a camada, podemos aplicá-la ao nosso grafo de exemplo. Primeiro vamos especificar algumas *features* dos vértices e a matriz de adjacência:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae815b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes features: \n",
      "tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n"
     ]
    }
   ],
   "source": [
    "nodes_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
    "print('Nodes features: \\n{}'.format(nodes_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "def66e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix: \n",
      "tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
    "                             [1, 1, 1, 1],\n",
    "                             [0, 1, 1, 1],\n",
    "                             [0, 1, 1, 1]]])\n",
    "print('Adjacency matrix: \\n{}'.format(adj_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc9c80",
   "metadata": {},
   "source": [
    "Agora vamos aplicar uma camada GCN. Para simplificar, inicializamos a matriz linear de pesos como uma matriz identidade para que as *features* de input sejam iguais as mensagens. Isso facilita a passagem delas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "332521a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix: \n",
      "tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "\n",
      "Input features: \n",
      "tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "\n",
      "Output features: \n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.],\n",
      "         [4., 5.],\n",
      "         [4., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GCNLayer(c_in=2, c_out=2)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(nodes_feats, adj_matrix)\n",
    "\n",
    "print('Adjacency matrix: \\n{}'.format(adj_matrix))\n",
    "print()\n",
    "print('Input features: \\n{}'.format(nodes_feats))\n",
    "print()\n",
    "print('Output features: \\n{}'.format(out_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb52e61",
   "metadata": {},
   "source": [
    "Como podemos observar, os valores de saída para o primeiro vértice são a média de si mesmo e do segundo vértice. Da mesma forma podemos verificar todos os outros vértices. No entando, em um GNN, também gostaríamos de permitir a troca de recursos entre os vértices além de seus vizinhos. Isso pode ser conseguido aplicando várias camadas de GCN, o que nos dá o layout final de uma GNN. A GNN pode ser construída por uma série de camadas GCN e não linearidades como a ReLU. Para visualização basta observar a figura abaixo (Tomas Kipf, 2016).\n",
    "\n",
    "![Arquitetura de uma GNN](./assets/gcn_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d7b2a",
   "metadata": {},
   "source": [
    "No entanto, podemos observar o seguinte, um problema na saída é que as saídas dos vértices 3 e 4 são os mesmos já que eles possuem os mesmos vértices adjacentes. Portanto as camadas GCN podem fazer os vértices esquecer suas informações específicas se apenas tomarmos uma média sobre todas as mensagens. Várias melhorias possíveis foram propostas ao longo dos anos. Enquanto a opção mais simples é colocar conexões residuais, a abordagem mais comum é avaliar as autoconexões mais alto ou definir uma matriz de peso separada para autoconexões. Como alternativa, podemos revisitar um conceito: *attention*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcfa224",
   "metadata": {},
   "source": [
    "### Graph Attention\n",
    "\n",
    "#### O que é *attention*?\n",
    "\n",
    "O mecanismo de *attention* descreve um novo grupo de camadas de redes neurais que tem atraído bastante interesse recentemente, especialmente para tarefas de sequências. Existem muitas definições de *attention* na literatura, mas a que melhor se encaixa nesse contexto é: o mecanismo de *attention* descreve uma média ponderada de (sequência) elementos com os pesos calculados dinamicamente com base em uma *input query* e as chaves dos elementos. Então o que isso quer dizer? O Objetivo é obter uma média das características de vários elementos. No entanto, em vez de ponderar cada elemento igualmente, queremos ponderar eles dependendo de seus valores reais. Em outras palavras, queremos decidir dinamicamente quais *inputs* queremos \"atender\" mais do que outras. Em geral, o mecanismo de *attention* tem quatro partes que precisamos especificar:\n",
    "\n",
    "- **Query**: A consulta é um vetor de *features* que descreve o que estamos procurando na sequência, ou seja, o que queremos prestar atenção.\n",
    "- **Keys**: Para cada elemento de entrada, temos uma chave que é novamente um vetor de *feature*. Este vetor de *features* descreve aproximadamente o que o elemento está oferecendo ou quando pode ser importante. As chaves devem ser projetadas de forma que possamos identificar os elementos os quais queremos prestar atenção com base na *query*.\n",
    "- **Values**: Para cada elemento de entrada, também temos um vetor de valores. Esse vetor é aquele sobre o qual queremos fazer a média.\n",
    "- **Score function**: Para classificar os elementos os quais queremos prestar atenção, devemos declarar uma função de pontuação, ou, *score function*. A função de pontuação recebe a *query* e uma chave como entrada e produz a pontuação/peso de atenção do par *query*-chave. Geralmente é implementado a partir de métricas de similaridade simples , como um produto escalar ou um pequeno MLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41340002",
   "metadata": {},
   "source": [
    "Os pesos da média são calculados por um softmax sobre todas as saídas da função de pontuação. Portanto, atribuímos a esses vetores de valor um peso maior, cuja chave correspondente é mais semelhante à consulta. Se tentamos descreve-lo com pseudo-matemática podemos escrever:\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{exp(f_{attn}(key_i, query)}{\\sum{_jexp(f_{attn}(key_j, query))}}, out = \\sum{\\alpha_i \\cdot value_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af539645",
   "metadata": {},
   "source": [
    "Visualmente podemos demonstrar o *attention* sobre uma sequência de palavras da seguinte maneira:\n",
    "\n",
    "![Demonstração do attention](./assets/attention_example.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ccd939",
   "metadata": {},
   "source": [
    "Para cada palavra temos uma chave e um valor. A consulta é comparada a todas as chaves com uma função de pontuação (neste caso, o produto escalar) para determinar os pesos. O softmax nesse caso não é visualizado a fim de simplificar o exemplo. Finalmente, os vetores de valor de todas as palavras são calculados usando pesos de *attention*.\n",
    "\n",
    "A maioria dos mecanismos de atenção difere em termos de quais consultas eles usam, como os vetores de chave e valor são definidos e qual função de pontuação é usada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce09ff4",
   "metadata": {},
   "source": [
    "Esse conceito pode ser aplicado de forma semelhante nos grafos, um deles é a *Graph Attention Network* (denominada GAT, proposta por [Velickovic et al., 2017](https://arxiv.org/abs/1710.10903)). Similar à GCN, a camada de atenção do grafo cria uma mensagem para cada nó usando uma camada linear/matriz de peso. Para a *attention part*, ele usa a mensagem do próprio vértice como uma consulta e as mensagens para calcular a média como chaves e valores (observe que isso também inclui a mensagem para ele mesmo). A função de pontuação $f_attn$ é implementada como um MLP de uma camada que mapeia a consulta e a chave para um único valor. O MLP tem a seguinte arquitetura (Velickovic et al.):\n",
    "\n",
    "![Arquitetura fattn mlp](./assets/graph_attention_MLP.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b7bf6",
   "metadata": {},
   "source": [
    "$h_i$ e $h_j$ são as *features* orginais dos vértices $i$ e $j$ respectivamente, e representam as mensagens da camada com o $W$ sendo a matriz de pesos. $a$ é a matriz de pesos da MLP, que tem um tamanho de $[1,2 x d_{message}]$, e $a_{ij}$ a o peso final do *attention* do vértice $i$ ao $j$. O cálculo pode ser descrito como o seguinte:\n",
    "\n",
    "$$\n",
    "a_ij = \\frac{exp(LeakyReLU(a[Wh_i||Wh_j]))}{\\sum_{k \\in N_i}exp(LeakyReLU(a[Wh_i||Wh_k]))}\n",
    "$$\n",
    "\n",
    "O operador $||$ representa concatenação, e $N_i$ representa os índices dos vizinhos ao vértice $i$. Observe que, em contraste com a prática usual, aplicamos uma não linearidade (aqui LeakyReLU) antes do softmax sobre os elementos. Embora pareça uma pequena alteração no início, é crucial que a atenção dependa da entrada original. Especificamente, vamos remover a não linearidade por um segundo e tentar simplificar a expressão:\n",
    "\n",
    "$$\n",
    "\\begin{split}\\begin{split}\n",
    "    \\alpha_{ij} & = \\frac{\\exp\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_j\\right]\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_k\\right]\\right)}\\\\[5pt]\n",
    "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i+\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i+\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\[5pt]\n",
    "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i\\right)\\cdot\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i\\right)\\cdot\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\[5pt]\n",
    "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\\n",
    "\\end{split}\\end{split}\n",
    "$$\n",
    "\n",
    "Podemos ver que sem a não linearidade, o termo de atenção com $h_i$ na verdade se anula, resultando na atenção sendo independente do próprio nó. Portanto, teríamos o mesmo problema que o GCN de criar os mesmos recursos de saída para nós com os mesmos vizinhos. É por isso que o LeakyReLU é crucial e adiciona alguma dependência de $h_i$ à atenção.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfbdbae",
   "metadata": {},
   "source": [
    "Depois de obter todos os fatores de *attention*, podemos calcular os recursos de saída para cada nó realizando a média ponderada:\n",
    "\n",
    "$$\n",
    "h_i'=\\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}\\mathbf{W}h_j\\right)\n",
    "$$\n",
    "\n",
    "$\\sigma$ é mais uma não linearidade, como na camada GCN. Visualmente, podemos representar a mensagem completa passando em uma camada de atenção da seguinte forma (Velickovic et al.):\n",
    "\n",
    "![Atenção em uma GCN](./assets/graph_attention.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a0ee86",
   "metadata": {},
   "source": [
    "Para aumentar a expressividade da Graph Attention Network, Velickovic et al. propôs estendê-lo a múltiplas *heads* similar ao bloco *Multi-Head Attention* em *Transformers*. Isso resulta em $N$ camadas de atenção sendo aplicadas em paralelo. Na imagem acima, ele é visualizado como três cores diferentes de setas (verde, azul e roxo) que são posteriormente concatenadas. A média é aplicada apenas para a camada de previsão final em uma rede."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad069c",
   "metadata": {},
   "source": [
    "Depois de discutir a camada de *attention* do grafo em detalhes, podemos implementá-la abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3e180dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimensão das features de entrada\n",
    "            c_out - Dimensão das features de saída\n",
    "            num_heads - Número de heads, i.e. mecanismos de attention que serão aplicados paralelamente.\n",
    "                        As features de saída são igualmente divididos entre os diferentes heads se concat_heads=True.\n",
    "            concat_heads - Se True, a saída dos diferentes heads é concatenado ao ínves de ser retirado a média. \n",
    "            alpha - inclanação negativa da ativação da LeakyReLU\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"Número de outputs deve ser múltiplo do número de heads\"\n",
    "            c_out = c_out // num_heads\n",
    "\n",
    "        #\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2* c_out))\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Features de entrada do vértice. Shape: [batch_size, c_in]\n",
    "            adj_matrix - Matriz de adjacência incluindo self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs - Se True, os pesos de attention são printados durante o forward (propositos de debbuging).\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "\n",
    "        # Aplica a camada linear e ordena os vértices pelo head\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "        # Precisamos calcular o attention logits para cada aresta na nossa matriz de adjacencia\n",
    "        # Fazer isso em todas as combinações possíveis é muito custoso\n",
    "        # => Cria um tensor de [W*h_i||W*h_j] com i e j sendo os índices de todos os vetores\n",
    "        edges = adj_matrix.nonzero(as_tuple=False) # Retorna os indices onde a matriz de adjacencia não é = 0\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:,0] * num_nodes + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * num_nodes + edges[:,2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1) # index_select retorna um tensor com node_feats_flat sendo indexado nas posições desejadas ao longo da dim=0\n",
    "\n",
    "        # Calcula a saída de attention da MLP\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "        \n",
    "        # Mapeia a lista de attentions de volta para uma matriz\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape + (self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[..., None].repeat(1, 1, 1, self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "        \n",
    "        # Média ponderada do attention\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print('Attention probs: \\n{}'.format(attn_probs.permute(0, 3, 1, 2)))\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "        \n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "        \n",
    "        return node_feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c060d4",
   "metadata": {},
   "source": [
    "Novamente, podemos aplicar a camada de attention do grafo em nosso grafo de exemplo para entender melhor a dinâmica. Como antes, a camada de entrada é inicializada como uma matriz de identidade, mas definimos $a$ como um vetor de números arbitrários para obter diferentes valores de atenção. Usamos dois heads para mostrar os mecanismos de attention independentes e paralelos que atuam na camada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66df452b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention probs: \n",
      "tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],\n",
      "          [0.1096, 0.1450, 0.2642, 0.4813],\n",
      "          [0.0000, 0.1858, 0.2885, 0.5257],\n",
      "          [0.0000, 0.2391, 0.2696, 0.4913]],\n",
      "\n",
      "         [[0.5100, 0.4900, 0.0000, 0.0000],\n",
      "          [0.2975, 0.2436, 0.2340, 0.2249],\n",
      "          [0.0000, 0.3838, 0.3142, 0.3019],\n",
      "          [0.0000, 0.4018, 0.3289, 0.2693]]]])\n",
      "\n",
      "Adjacency matrix \n",
      "tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "\n",
      "Input features \n",
      "tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "\n",
      "Output features \n",
      "tensor([[[1.2913, 1.9800],\n",
      "         [4.2344, 3.7725],\n",
      "         [4.6798, 4.8362],\n",
      "         [4.5043, 4.7351]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GATLayer(2, 2, num_heads=2)\n",
    "\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "layer.a.data = torch.Tensor([[-0.2, 0.3], [0.1, -0.1]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(nodes_feats, adj_matrix, print_attn_probs=True)\n",
    "\n",
    "print()\n",
    "print(\"Adjacency matrix \\n{}\".format(adj_matrix))\n",
    "print()\n",
    "print(\"Input features \\n{}\".format(nodes_feats))\n",
    "print()\n",
    "print(\"Output features \\n{}\".format(out_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5399805",
   "metadata": {},
   "source": [
    "Recomendamos que você tente calcular a matriz de atenção pelo menos para um head e um vértice você mesmo. As entradas são 0 onde não existe uma aresta entre i e j. Para os outros, vemos um conjunto diversificado de probabilidades de atenção. Além disso, os recursos de saída dos nós 3 e 4 agora são diferentes, embora tenham os mesmos vizinhos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
